[suzuki_test]$ head -n 3 509_1_CustomerMaster_All_20250916.csv
"0","TOP","232","20250916","0","00000","000","000",,,,
"1","DATA","G000126255","損害保険ジャパン（株）","ｿﾝｶﾞｲﾎｹﾝｼﾞﾔﾊﾟﾝ","SOMPO JAPAN INSURANCE INC.",,"1000","690548136","4011101023372","524126","160-0023","東京 都","新宿区","西新宿１－２６－１","損害保険ジャパンビル","SONGAI HOKEN JAPAN BLDG.","1-26-1, NISHISHINJUKU","SHINJUKU-KU","TOKYO","JAPAN","5000000009","本社 　Ｂ４","ホンシヤ","541-0048","大阪府大阪市中央区瓦町４－１－２","損保ジャパン大阪ビル",,"株式会社損害保険ジャパン",,"JP","JA","06-6227-4366","0","JPY","691152841","A3000380000050","9910013",,,,,"20250911T000000+0000","99991231T000000+0000","20250911T000000+0000"
"2","DATA","G000126255","損害保険ジャパン（株）","ｿﾝｶﾞｲﾎｹﾝｼﾞﾔﾊﾟﾝ","SOMPO JAPAN INSURANCE INC.",,"1000","690548136","4011101023372","524126","160-0023","東京 都","新宿区","西新宿１－２６－１","損害保険ジャパンビル","SONGAI HOKEN JAPAN BLDG.","1-26-1, NISHISHINJUKU","SHINJUKU-KU","TOKYO","JAPAN","5000000010","金沢 保険サービス課","カナザワホケンサービスカ","920-8558","石川県金沢市香林坊１－２－２１",,,"損害保険ジャパン日本興亜　株式会社","金沢保険サービス課","JP","JA","076-232-2435","0","JPY","695141234","A3000380000123","9933743",,,,,"20250911T000000+0000","99991231T000000+0000","20250911T000000+0000"
[ftp_admin@d-mdm-frs01 suzuki_test]$ tail -n 3 509_1_CustomerMaster_All_20250916.csv
"91870","DATA","G900000094","小口資金（ＮＴＴファシリティーズ中国）",,,,"1000",,,,,,,,,,,,,,"9000003348","（株）ＮＴＴファシリティーズ中国","ホンシャ","730-0004","広島県広島市中区東白島町１４－１５","　ＮＴＴクレド白島ビル",,"株式会社ＮＴＴファシリティーズ中国","（株）ＮＴＴファシリティーズ中国","JP","JA","082-212-3473","0","JPY",,,,,,,,"20230113T000000+0000","99991231T000000+0000","20230113T000000+0000"
"91871","DATA","G900000120","対向システム用",,,,"1000",,,,,,,,,,,,,,"9000003475","対向システム用＿仕訳エンジン","タイコウシステムヨウ＿シワケエンジン","000-0000","＊＊＊＊",,,"＊＊＊＊",,"JP","JA","000-000-0000","0","JPY",,,,,,,,"20230406T000000+0000","99991231T000000+0000","20230407T000000+0000"
"91872","END","91873"
[suzuki_test]$


#!/bin/bash

#================================================================
# CSVフィルタリングツール (TOP/END, リストファイル, 複合CSV対応版)
#
# 説明:
#   指定された列の値が、指定された文字列(またはリストファイル)のいずれかと
#   一致するデータ行を抽出します。
#   ファイルの先頭にあるTOPレコードと末尾にあるENDレコードを自動的に処理し、
#   抽出後のデータに合わせて1列目のカウンタを再附番し、ENDレコードの
#   総レコード数を更新します。
#   空フィールドや、フィールド内にカンマを含むCSVレコードも正しく処理します。
#
# 依存関係:
#   gawk (GNU Awk) - CSVモード(PROCINFO["csv"])を使用するため
#
# 入力CSVの前提フォーマット:
#   "0","TOP",...
#   "1","DATA",...
#   ...
#   "N","END","N+1"
#
# 使用方法:
#   ./filter_csv.sh -c <列番号> {-s <文字列> | -f <リストファイル>} [file1.csv ...]
#
# オプション:
#   -c <列番号>        : フィルタリング対象のカラム番号。必須。
#   -s <文字列>        : 抽出する文字列をカンマ区切りで指定 (例: "G111,G3333")。
#   -f <リストファイル> : 抽出する文字列を1行1件で記述したファイル。
#   -h                 : このヘルプメッセージを表示する。
#
#================================================================

# --- デフォルト設定 ---
COLUMN=""
MATCH_STRINGS=""
LIST_FILE=""

# --- ヘルプメッセージ表示関数 ---
usage() {
    echo "使用方法: $0 -c <列番号> {-s <文字列> | -f <リストファイル>} [file1.csv ...]"
    echo "  -c <列番号>        : フィルタリング対象のカラム番号。必須。"
    echo "  -s <文字列>        : 抽出する文字列をカンマ区切りで指定 (例: \"G111,G3333\")。"
    echo "  -f <リストファイル> : 抽出する文字列を1行1件で記述したファイル。"
    echo "  -h                 : このヘルプメッセージを表示する。"
    exit 1
}

# --- オプション解析 ---
while getopts "c:s:f:h" opt; do
    case ${opt} in
        c)
            COLUMN=${OPTARG}
            ;;
        s)
            MATCH_STRINGS=${OPTARG}
            ;;
        f)
            LIST_FILE=${OPTARG}
            ;;
        h)
            usage
            ;;
        \?)
            usage
            ;;
    esac
done
shift $((OPTIND - 1))

# --- 必須引数の検証 ---
if [ -z "${COLUMN}" ]; then
    echo "エラー: -c は必須オプションです。" >&2
    usage
fi

if [ -n "${MATCH_STRINGS}" ] && [ -n "${LIST_FILE}" ]; then
    echo "エラー: -s と -f オプションは同時に指定できません。" >&2
    usage
fi

if [ -z "${MATCH_STRINGS}" ] && [ -z "${LIST_FILE}" ]; then
    echo "エラー: -s または -f のどちらか一方を指定してください。" >&2
    usage
fi

if [ -n "${LIST_FILE}" ] && [ ! -f "${LIST_FILE}" ]; then
    echo "エラー: リストファイルが見つかりません: ${LIST_FILE}" >&2
    exit 1
fi

if [ $# -eq 0 ]; then
    echo "エラー: 処理対象のファイルが指定されていません。" >&2
    usage
fi

# --- メイン処理ループ ---
for file in "$@"; do
    if [ ! -f "$file" ]; then
        echo "警告: ファイルが見つかりません。スキップします: $file" >&2
        continue
    fi

    if [ $(wc -l < "$file") -lt 2 ]; then
        echo "警告: ファイルの行数が2行未満です。スキップします: $file" >&2
        continue
    fi

    # awkを使用してCSVをフィルタリングおよび再構成
    awk -v col="${COLUMN}" \
        -v patterns="${MATCH_STRINGS}" \
        -v listfile="${LIST_FILE}" \
    '
    BEGIN {
        # gawkのCSVモードを有効にする (RFC 4180準拠)。これにより空フィールドも正しく扱える。
        PROCINFO["csv"] = 1
        OFS = ","

        # 抽出条件を match_targets 連想配列に格納する
        if (listfile != "") {
            while ((getline line < listfile) > 0) {
                # 行の前後の空白を削除
                gsub(/^[ \t]+|[ \t]+$/, "", line)
                # 空行はスキップ
                if (line == "") continue
                match_targets[line] = 1
            }
            close(listfile)
        }
        else if (patterns != "") {
            split(patterns, pattern_array, ",")
            for (p in pattern_array) {
                match_targets[pattern_array[p]] = 1
            }
        }
    }

    # 全ての行を配列に格納
    { lines[FNR] = $0 }

    END {
        # --- TOP/ENDレコードの形式を検証 ---
        # $0に代入することで、gawkにCSVとしてパースさせる
        $0 = lines[1]
        if ($2 != "TOP") {
            print "エラー: 1行目がTOPレコードではありません。ファイル:", FILENAME > "/dev/stderr"
            exit 1
        }
        $0 = lines[FNR]
        if ($2 != "END") {
            print "エラー: 最終行がENDレコードではありません。ファイル:", FILENAME > "/dev/stderr"
            exit 1
        }

        # 1. TOPレコードはそのまま出力
        print lines[1]

        # 2. データ行 (2行目から最終-1行目) をフィルタリング
        data_counter = 0
        if (FNR > 2) {
            for (i = 2; i < FNR; i++) {
                # gawkのCSVモードでパースするために$0にセットする
                $0 = lines[i]
                
                # CSVモードではフィールドのクォートは自動で除去されるため、$colはクォートなしの値になる
                field_to_check = $col

                # 抽出対象の文字列と一致するかチェック
                if (field_to_check in match_targets) {
                    data_counter++
                    
                    # 元の行のフォーマットを維持するため、パース前の行に対してカウンタ部分のみを置換
                    modified_line = lines[i]
                    sub(/^"[^"]*"/, "\"" data_counter "\"", modified_line)
                    print modified_line
                }
            }
        }

        # 3. 新しいENDレコードを生成して出力
        total_records = 1 + data_counter + 1
        end_counter = total_records - 1
        
        printf "\"%d\",\"END\",\"%d\"\n", end_counter, total_records
    }
    ' "$file"
done
